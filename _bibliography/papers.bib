@inproceedings{beugnot2021beyond,
  title     = {Beyond Tikhonov: faster learning with self-concordant losses, via iterative regularization},
  author    = {Beugnot, Gaspard and Mairal, Julien and Rudi, Alessandro},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  year      = {2021},
  url       = {https://openreview.net/forum?id=eIdzV1-Jdwv},
  abstract  = {The theory of spectral filtering is a remarkable tool to understand the statistical properties of learning with kernels. For least squares, it allows to derive various regularization schemes that yield faster convergence rates of the excess risk than with Tikhonov regularization. This is typically achieved by leveraging classical assumptions called source and capacity conditions, which characterize the difficulty of the learning task. In order to understand estimators derived from other loss functions, Marteau-Ferey et al. have extended the theory of Tikhonov regularization to generalized self concordant loss functions (GSC), which contain, e.g., the logistic loss. In this paper, we go a step further and show that fast and optimal rates can be achieved for GSC by using the iterated Tikhonov regularization scheme, which is intrinsically related to the proximal point method in optimization, and overcomes the limitation of the classical Tikhonov regularization.},
  selected  = {true},
  html      = {https://openreview.net/forum?id=eIdzV1-Jdwv},
  pdf       = {https://proceedings.neurips.cc/paper/2021/file/eda80a3d5b344bc40f3bc04f65b7a357-Paper.pdf},
  conf      = {NeurIPS21},
  award     = {Spotlight},
  video     = {https://nips.cc/virtual/2021/poster/26184}
}

@inproceedings{beugnot21ot,
  title     = {Improving approximate optimal transport distances using quantization},
  author    = {Beugnot, Gaspard and Genevay, Aude and Greenewald, Kristjan and Solomon, Justin},
  booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages     = {290--300},
  year      = {2021},
  editor    = {de Campos, Cassio and Maathuis, Marloes H.},
  volume    = {161},
  series    = {Proceedings of Machine Learning Research},
  month     = {27--30 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v161/beugnot21a/beugnot21a.pdf},
  url       = {https://proceedings.mlr.press/v161/beugnot21a.html},
  abstract  = {Optimal transport (OT) is a popular tool in machine learning to compare probability measures geometrically, but it comes with substantial computational burden. Linear programming algorithms for computing OT distances scale cubically in the size of the input, making OT impractical in the large-sample regime. We introduce a practical algorithm, which relies on a quantization step, to estimate OT distances between measures given cheap sample access. We also provide a variant of our algorithm to improve the performance of approximate solvers, focusing on those for entropy-regularized transport. We give theoretical guarantees on the benefits of this quantization step and display experiments showing that it behaves well in practice, providing a practical approximation algorithm that can be used as a drop-in replacement for existing OT estimators.},
  selected  = {false},
  conf      = {UAI21}
}

@misc{beugnot22stepsizeconvex,
  doi       = {10.48550/ARXIV.2202.13733},
  url       = {https://arxiv.org/abs/2202.13733},
  author    = {Beugnot, Gaspard and Mairal, Julien and Rudi, Alessandro},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title     = {On the Benefits of Large Learning Rates for Kernel Methods},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  abstract  = {This paper studies an intriguing phenomenon related to the good generalization performance of estimators obtained by using large  learning rates within gradient descent algorithms. First observed in the deep learning literature, we show that a phenomenon can be precisely characterized in the context of kernel methods, even though the resulting optimization problem is convex. Specifically, we consider the minimization of a quadratic objective in a separable Hilbert space, and show that with early stopping, the choice of learning rate influences the spectral decomposition of the obtained solution on the Hessian's eigenvectors. This extends an intuition described by Nakkiran (2020) on a two-dimensional toy problem to realistic learning scenarios such as kernel ridge regression. While large learning rates may be proven beneficial as soon as there is a mismatch between the train and test objectives, we further explain why it already occurs in classification tasks without assuming any particular mismatch between train and test data distributions.},
  pdf       = {https://arxiv.org/pdf/2202.13733.pdf},
  selected  = {true},
  conf      = {COLT22},
  arxiv     = {https://arxiv.org/abs/2202.13733}
}